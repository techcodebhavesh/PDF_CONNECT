{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] =\" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d7d995-7beb-40b5-9a44-afd350b7d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wife of a rich man fell sick, and as she felt that her end\n",
      "was drawing near, she called her only\n"
     ]
    }
   ],
   "source": [
    "# # Cinderella story defined in sample.txt\n",
    "# with open('demo/sample.txt', 'r') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:00:52,271 - Loading faiss with AVX512 support.\n",
      "2025-01-27 12:00:52,273 - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-01-27 12:00:52,274 - Loading faiss with AVX2 support.\n",
      "2025-01-27 12:00:52,297 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "from raptor import RetrievalAugmentation \n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e843edf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalAugmentation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m RA \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalAugmentation\u001b[49m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# construct the tree\u001b[39;00m\n\u001b[0;32m      4\u001b[0m RA\u001b[38;5;241m.\u001b[39madd_documents(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RetrievalAugmentation' is not defined"
     ]
    }
   ],
   "source": [
    "# RA = RetrievalAugmentation()\n",
    "\n",
    "# # construct the tree\n",
    "# RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
   "metadata": {},
   "source": [
    "### Querying from the tree\n",
    "\n",
    "```python\n",
    "question = # any question\n",
    "RA.answer_question(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How did Cinderella reach her happy ending ?\"\n",
    "\n",
    "# answer = RA.answer_question(question=question)\n",
    "\n",
    "# print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the tree by calling RA.save(\"path/to/save\")\n",
    "# SAVE_PATH = \"demo/cinderella\"\n",
    "# RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load back the tree by passing it into RetrievalAugmentation\n",
    "\n",
    "# RA = RetrievalAugmentation(tree=SAVE_PATH)\n",
    "\n",
    "# answer = RA.answer_question(question=question)\n",
    "# print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model_name=\"llama-3.3-70b-versatile\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Write a summary of the following, including as many key details as possible: {context}\",\n",
    "            }\n",
    "        ]\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self, model_name=\"llama-3.3-70b-versatile\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Given Context: {context} Give the best full answer to the question {question}\",\n",
    "            }\n",
    "        ]\n",
    "        # Remove unsupported arguments (e.g., top_k, top_p)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=256,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255791ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:01:40,067 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
      "2025-01-27 12:01:40,349 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:01:59,891 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x0000021B61650560>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x0000021B61651730>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-01-27 12:01:59,892 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x0000021B61650560>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x0000021B61651730>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-01-27 12:01:59,893 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x0000021B61650560>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x0000021B61651730>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x0000021B61651730>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.GEMMAQAModel object at 0x0000021B61650D40>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:02,544 - Creating Leaf Nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624a5619d4bd4958ba807c2da6a8b33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a875dbf35a4be19d4cf83978159c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bf01bc89aa4ef8a05ac73af541e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acf327617254009bd2926d72b5cb3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2351c19be84066aa1dfe6cb562ba48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e78d8640af640b0ac9e033ab925fc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff1c298e9534d11bddf5983c73d8ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef40a5a757c2401d9fdafa6cf7ceefb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b9d77c3443452f898f81407f847301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c995e9e517bc4d298605472d167c29fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f733692a3264d64bae0bc31a74fd739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef356a1020e4fb6baaa06a15b0f8e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e995c5f78d34ca0a7f367bdb2b1d683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84835f762ab4900ac4e7ef393860a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b469f75287d840c9a37ea350cba9bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81061b041c249c380c925db80135a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db63f950296c44c19457e23ca9cec0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053a15811cf54f078163b21f5c8a8481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d3575e52a24c86881ffaa6f2e9ed98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ebb2badc114d86bca280bf6d058628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b8353547a9489d980bd4f3e5610fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3069115a0c6f400da7812adb636d8c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:12,390 - Created 22 Leaf Embeddings\n",
      "2025-01-27 12:02:12,390 - Building All Nodes\n",
      "2025-01-27 12:02:12,392 - Using Cluster TreeBuilder\n",
      "2025-01-27 12:02:12,393 - Constructing Layer 0\n",
      "2025-01-27 12:02:20,087 - Summarization Length: 100\n",
      "2025-01-27 12:02:21,187 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 12:02:21,193 - Node Texts Length: 580, Summarized Text Length: 101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d881e2d89804d6ca2ef92c30e9837f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:22,761 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 12:02:22,766 - Node Texts Length: 377, Summarized Text Length: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de9bf0d96d041a68cf25f81c9b1230f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:23,908 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 12:02:23,914 - Node Texts Length: 369, Summarized Text Length: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687aa6e7d568477caafbab5c444f37b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:24,929 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 12:02:24,931 - Node Texts Length: 368, Summarized Text Length: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b8a44938a8454b840d5460d234eef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:25,911 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 12:02:25,913 - Node Texts Length: 366, Summarized Text Length: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eedf911d87c41b89345bfe53e82c02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:02:26,053 - Constructing Layer 1\n",
      "2025-01-27 12:02:26,053 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 1\n",
      "2025-01-27 12:02:26,054 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x0000021B61651730>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "with open('D:/PDF_CONNECT/raptor/csx.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "    text = file.read()\n",
    "\n",
    "RA.add_documents(text)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:03:16,600 - Using collapsed_tree\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b484ab1a4daf42a5b2d506c17818112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 12:03:18,257 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  To create a Material Master at the company code level, you can follow these steps using the streamlined transaction ZCML:\n",
      "\n",
      "**Step 1: Access the ZCML Transaction**\n",
      "\n",
      "* Log in to your SAP system and navigate to the ZCML transaction.\n",
      "* You can search for the transaction code ZCML in the SAP command prompt or navigate to it through the SAP menu.\n",
      "\n",
      "**Step 2: Enter Material Master Data**\n",
      "\n",
      "* On the ZCML initial screen, enter the material number, plant ID, and other required details.\n",
      "* The plant ID is used to create views such as sales plant, foreign views, purchasing view, MRP views, costing/accounting views, and storage/warehouse views.\n",
      "* You can also copy defaults from an existing MRP profile or data from an existing material number.\n",
      "\n",
      "**Step 3: Enter Sales and Purchasing Data**\n",
      "\n",
      "* Enter the sales organization, distribution channel, and warehouse number (if required).\n",
      "* Assign a purchasing organization for externally procured materials.\n",
      "\n",
      "**Step 4: Review and Modify Proposed Values**\n",
      "\n",
      "* The ZCML transaction will propose default values based on data from the Tyco Electronics Data warehouse (TED), Transfer Pricing Excellence database (TPEX), and other sources.\n",
      "* Review and modify these proposed\n"
     ]
    }
   ],
   "source": [
    "question = \"how to  create Create Material Master – company code level\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4297729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
